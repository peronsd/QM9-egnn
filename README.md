# QM9-egnn
Задача предсказания HOMO-LUMO gap на датасете QM9.
Для начала была построена простая MLP модель на 3 слоя без дополнительного анализа данных. Модель обучена и протестирована, результаты неплохие, но слишком далеки от идеала.
Попутно в качестве эксперимента при помощи библиотеки CatBoost была построена модель градиентного бустинга на 2000 деревьев, но она дала совсем не приятные результаты, поэтому использовать ее в сравнении не имеет смысла.
Теперь перейдем к основной идее проекта. При помощи библиотеки egnn-pytorch были построены две эквивариантные нейронные сети: одна на исходных данных, другая на них же но с дополнительными фичами топологического анализа.
# Топологических фичи
* Комплекс Виеториса-Рипса, для извлечения групп гомологий H0 (компоненты связности) и H1 (циклы). Когда ребра (1-симплексы) образуют петлю, но внутри нее нет 2-сиплексов, закрывающих эту дыру — возникает цикл. Группу H2 строить не стал уже.
* Устойчивые гомологии (Persistent Homology), для построения диаграммы устойчивости, показывающей «время жизни» признака. 
* Полученную диаграмму устойчивости трансформировали в «устойчивое изображение». Для группы H0 показывает, как быстро атомы сливаются в группы на разных масштабах. Для группы H1 показывает стабильность циклов.
* Кривые Бетти. По сути показывают количество структур на дистанции.
# EGNN
Архитектура сети собрана самостоятельно: один линейный слой в качестве эмбеддинга, два эквивариантных графовых слоя и один выходной регрессор, состоящий из трех линейных слоев с сигмоидами в качестве активаций.
# MLP и EGNN
Для сравнения моделей везде строилась метрика MAE, так как ее удобнее всего интерпретировать (ошибка измеряется так же в eV как и целевая переменная)
Простая сеть показала себя относительно неплохо, хотя ошибка и достаточно велика. Применение графовой сети самой по себе без подготовки данных уже дает весомый прирост к качеству модели, что можно увидеть на графиках "вранья" моделей в блокнотах.
# EGNN и EGNN + TDA
Во-первых, модель с TDA сходится заметно дольше модели без него, поэтому дадим небольшую фору первой, обучив ее на *1.5 эпохах. Несмотря на это EGNN + TDA остается более шумной (нужно ее еще больше обучать), однако на многих запусках прослеживается тенденция: сеть с TDA выдает более "равномерную ошибку" (у сети без TDA прослеживается "тружнопритягиваемый" квадрат [6;7]x[5;6] (на рис. в блокноте), у сети с TDA нет таких бугров).
# Выводы
Построены несколько работоспособных сетей. EGNN показывает себя намного лучше простого многослойного персептрона и модели машинного обучения с бустингом. Один из наилучших полученных мной результатов средней абсолютной ошибки составил 0.13 eV, что считаю неплохим результатом. Результат этот еще улучшаем, например, увеличением количества эпох обучения (за 50 эпох я не достиг апогея лосса).
